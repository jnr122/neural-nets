{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Reinforcement Learning with OpenAI Gym\n",
    "---\n",
    "This notebook will create and test different reinforcement learning agents and environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Environment\n",
    "---\n",
    "Call `gym.make(\"environment name\")` to load a new environment.\n",
    "\n",
    "Check out the list of available environments at <https://gym.openai.com/envs/>\n",
    "\n",
    "Edit this cell to load different environments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 12:41:07,489] Making new env: Assault-ram-v0\n"
     ]
    }
   ],
   "source": [
    "# TODO: Load an environment\n",
    "env = gym.make(\"Assault-ram-v0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(128,)\nDiscrete(7)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Print observation and action spaces\n",
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run an Agent\n",
    "---\n",
    "\n",
    "Reset the environment before each run with `env.reset`\n",
    "\n",
    "Step forward through the environment to get new observations and rewards over time with `env.step`\n",
    "\n",
    "`env.step` takes a parameter for the action to take on this step and returns the following:\n",
    "- Observations for this step\n",
    "- Rewards earned this step\n",
    "- \"Done\", a boolean value indicating if the game is finished\n",
    "- Info - some debug information that some environments provide. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357.0\n"
     ]
    }
   ],
   "source": [
    "# TODO Make a random agent\n",
    "games_to_play = 10\n",
    "\n",
    "for i in range(games_to_play):\n",
    "    obs = env.reset()\n",
    "    episode_rewards = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        env.render()\n",
    "        time.sleep(0.02)\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        episode_rewards += reward\n",
    "\n",
    "    print(episode_rewards)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy Gradients\n",
    "---\n",
    "The policy gradients algorithm records gameplay over a training period, then runs the results of the actions chosen through a neural network, making successful actions that resulted in a reward more likely, and unsuccessful actions less likely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Build the policy gradient neural network\n",
    "class Agent:\n",
    "    def __init__(self, num_actions, state_size):\n",
    "        \n",
    "        initializer = tf.contrib.layers.xavier_initializer()\n",
    "        \n",
    "        self.input_layer = tf.placeholder(dtype=tf.float32, shape=[None, state_size])\n",
    "        \n",
    "        # Neural net starts here\n",
    "        \n",
    "        hidden_layer = tf.layers.dense(self.input_layer, 8, activation=tf.nn.relu, kernel_initializer=initializer)\n",
    "        hidden_layer_2 = tf.layers.dense(hidden_layer, 8, activation=tf.nn.relu, kernel_initializer=initializer)\n",
    "        hidden_layer_3 = tf.layers.dense(hidden_layer_2, 8, activation=tf.nn.relu, kernel_initializer=initializer)\n",
    "        hidden_layer_4 = tf.layers.dense(hidden_layer_3, 8, activation=tf.nn.relu, kernel_initializer=initializer)\n",
    "        hidden_layer_5 = tf.layers.dense(hidden_layer_4, 8, activation=tf.nn.relu, kernel_initializer=initializer)\n",
    "        hidden_layer_6 = tf.layers.dense(hidden_layer_5, 8, activation=tf.nn.relu, kernel_initializer=initializer)\n",
    "        hidden_layer_7 = tf.layers.dense(hidden_layer_6, 8, activation=tf.nn.relu, kernel_initializer=initializer)\n",
    "        hidden_layer_8 = tf.layers.dense(hidden_layer_7, 8, activation=tf.nn.relu, kernel_initializer=initializer)\n",
    "\n",
    "        \n",
    "        # Output of neural net\n",
    "        out = tf.layers.dense(hidden_layer_8, num_actions, activation=None)\n",
    "        \n",
    "        self.outputs = tf.nn.softmax(out)\n",
    "        self.choice = tf.argmax(self.outputs, axis=1)\n",
    "        \n",
    "        # Training Procedure\n",
    "        self.rewards = tf.placeholder(shape=[None, ], dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=[None, ], dtype=tf.int32)\n",
    "        \n",
    "        one_hot_actions = tf.one_hot(self.actions, num_actions)\n",
    "        \n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=out, labels=one_hot_actions)\n",
    "        \n",
    "        self.loss = tf.reduce_mean(cross_entropy * self.rewards)\n",
    "        \n",
    "        self.gradients = tf.gradients(self.loss, tf.trainable_variables())\n",
    "        \n",
    "        # Create a placeholder list for gradients\n",
    "        self.gradients_to_apply = []\n",
    "        for index, variable in enumerate(tf.trainable_variables()):\n",
    "            gradient_placeholder = tf.placeholder(tf.float32)\n",
    "            self.gradients_to_apply.append(gradient_placeholder)\n",
    "            \n",
    "        # Create the operation to update gradients with the gradients placeholder.\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=1e-2)\n",
    "        self.update_gradients = optimizer.apply_gradients(zip(self.gradients_to_apply, tf.trainable_variables()))\n",
    "        self.update_gradients = optimizer.apply_gradients(zip(self.gradients_to_apply, tf.trainable_variables()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discounting and Normalizing Rewards\n",
    "---\n",
    "In order to determine how \"successful\" a given action is, the policy gradient algorithm evaluates each action based on how many rewards were earned after it was performed in an episode.\n",
    "\n",
    "The discount rewards function goes through each time step of an episode and tracks the total rewards earned from each step to the end of the episode.\n",
    "\n",
    "For example, if an episode took 10 steps to finish, and the agent earns 1 point of reward every step, the rewards for each frame would be stored as \n",
    "`[10, 9, 8, 7, 6, 5, 4, 3, 2, 1]`\n",
    "\n",
    "This allows the agent to credit early actions that didn't lose the game with future success, and later actions (that likely resulted in the end of the game) to get less credit.\n",
    "\n",
    "One disadvantage of arranging rewards like this is that early actions didn't necessarily directly contribute to later rewards, so a **discount factor** is applied that scales rewards down over time. A discount factor < 1 means that rewards earned closer to the current time step will be worth more than rewards earned later.\n",
    "\n",
    "With our reward example above, if we applied a discount factor of .90, the rewards would be stored as\n",
    "`[ 6.5132156   6.12579511  5.6953279   5.217031    4.68559     4.0951      3.439\n",
    "  2.71        1.9         1. ]`\n",
    "\n",
    "This means that the early actions still get more credit than later actions, but not the full value of the rewards for the entire episode.\n",
    "\n",
    "Finally, the rewards are normalized to lower the variance between reward values in longer or shorter episodes.\n",
    "\n",
    "You can tweak the discount factor as one of the hyperparameters of your model to find one that fits your task the best!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Create the discounted and normalized rewards function\n",
    "discount_rate = 0.95\n",
    "\n",
    "def discount_normalize_rewards(rewards):\n",
    "    discounted_rewards = np.zeros_like(rewards)\n",
    "    total_rewards = 0\n",
    "    \n",
    "    for i in reversed(range(len(rewards))):\n",
    "        total_rewards = total_rewards * discount_rate + rewards[i]\n",
    "        discounted_rewards[i] = total_rewards\n",
    "        \n",
    "    discounted_rewards -= np.mean(discounted_rewards)\n",
    "    discounted_rewards /= np.std(discounted_rewards)\n",
    "    \n",
    "    return discounted_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Procedure\n",
    "---\n",
    "The agent will play games and record the history of the episode. At the end of every game, the episode's history will be processed to calculate the **gradients** that the model learned from that episode.\n",
    "\n",
    "Every few games the calculated gradients will be applied, updating the model's parameters with the lessons from the games so far.\n",
    "\n",
    "While training, you'll keep track of average scores and render the environment occasionally to see your model's progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward / 100 eps: -200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward / 100 eps: -200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward / 100 eps: -200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward / 100 eps: -200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward / 100 eps: -200.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-1566809da340>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.02\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;31m# Get weights for each action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# TODO Create the training loop\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Modify these to match shape of actions and states in your environment\n",
    "num_actions = 4\n",
    "state_size = 128\n",
    "\n",
    "path = \"./MsPacman-ram-pg/\"\n",
    "\n",
    "training_episodes = 1000\n",
    "max_steps_per_episode = 10000\n",
    "episode_batch_size = 5\n",
    "\n",
    "agent = Agent(num_actions, state_size)\n",
    " \n",
    "init = tf.global_variables_initializer()\n",
    " \n",
    "saver = tf.train.Saver(max_to_keep=2)\n",
    " \n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    " \n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "     \n",
    "    total_episode_rewards = []\n",
    "    \n",
    "    # Create a buffer of 0'd gradients\n",
    "    gradient_buffer = sess.run(tf.trainable_variables())\n",
    "    for index, gradient in enumerate(gradient_buffer):\n",
    "        gradient_buffer[index] = gradient * 0\n",
    "        \n",
    "    for episode in range(training_episodes):\n",
    " \n",
    "        state = env.reset()\n",
    "         \n",
    "        episode_history = []\n",
    "        episode_rewards = 0\n",
    "        \n",
    "        for step in range(max_steps_per_episode):\n",
    "             \n",
    "            if episode % 100 == 0:\n",
    "                env.render()\n",
    "                time.sleep(0.02)\n",
    "             \n",
    "            # Get weights for each action\n",
    "            action_probabilities = sess.run(agent.outputs, feed_dict={agent.input_layer: [state]})\n",
    "            action_choice = np.random.choice(range(num_actions), p=action_probabilities[0])\n",
    "             \n",
    "            state_next, reward, done, _ = env.step(action_choice)\n",
    "            episode_history.append([state, action_choice, reward, state_next])\n",
    "            state = state_next\n",
    "             \n",
    "            episode_rewards += reward\n",
    "            \n",
    "            if done or step + 1 == max_steps_per_episode:\n",
    "                total_episode_rewards.append(episode_rewards)\n",
    "                episode_history = np.array(episode_history)\n",
    "                episode_history[:,2] = discount_normalize_rewards(episode_history[:,2])\n",
    "                \n",
    "                ep_gradients = sess.run(agent.gradients, feed_dict={agent.input_layer: np.vstack(episode_history[:,0]), agent.actions: episode_history[:,1], agent.rewards: episode_history[:,2]})\n",
    "               \n",
    "               \n",
    "                # add the gradients to the grad buffer: \n",
    "                for index, gradient in enumerate(ep_gradients):\n",
    "                    gradient_buffer[index] += gradient\n",
    "                    \n",
    "                break\n",
    "                \n",
    "        if episode % episode_batch_size == 0:\n",
    "            \n",
    "            feed_dict_gradients = dict(zip(agent.gradients_to_apply, gradient_buffer))\n",
    "            \n",
    "            sess.run(agent.update_gradients, feed_dict=feed_dict_gradients)\n",
    "            \n",
    "            for index, gradient in enumerate(gradient_buffer):\n",
    "                gradient_buffer[index] = gradient * 0\n",
    "                \n",
    "        if episode % 100 == 0:\n",
    "            saver.save(sess, path + \"pg-checkpoint\", episode)\n",
    "            print(\"Average reward / 100 eps: \" + str(np.mean(total_episode_rewards[-100:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the Model\n",
    "---\n",
    "\n",
    "This cell will run through games choosing actions without the learning process so you can see how your model has learned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./MountainCar-pg/pg-checkpoint-900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-07-25 11:28:29,763] Restoring parameters from ./MountainCar-pg/pg-checkpoint-900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards for episode0: -200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards for episode1: -200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards for episode2: -200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards for episode3: -200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards for episode4: -200.0\n"
     ]
    }
   ],
   "source": [
    "# TODO Create the testing loop\n",
    "testing_episodes = 5\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    checkpoint = tf.train.get_checkpoint_state(path)\n",
    "    saver.restore(sess,checkpoint.model_checkpoint_path)\n",
    "\n",
    "    for episode in range(testing_episodes):\n",
    "\n",
    "        state = env.reset()\n",
    "\n",
    "        episode_rewards = 0\n",
    "\n",
    "        for step in range(max_steps_per_episode):\n",
    "\n",
    "            env.render()\n",
    "            time.sleep(0.02)\n",
    "\n",
    "            # Get Action\n",
    "            action_argmax = sess.run(agent.choice, feed_dict={agent.input_layer: [state]})\n",
    "            action_choice = action_argmax[0]\n",
    "\n",
    "            state_next, reward, done, _ = env.step(action_choice)\n",
    "            state = state_next\n",
    "\n",
    "            episode_rewards += reward\n",
    "\n",
    "            if done or step +1 == max_steps_per_episode:\n",
    "                print(\"Rewards for episode\" + str(episode) + \": \" + str(episode_rewards ))\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
